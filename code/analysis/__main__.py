import os, argparse
from typing import List
from time import time
import numpy as np

import analysis.loading as load
import analysis.origin_search as origin
import analysis.eval as eval
import analysis.graph as graph

def init_report(path_report : str):
    """
    Initialize report file with fields.
    """
    with open(path_report, 'a+') as file:
        file.write(f"Sp_sending\t\tSending_position\t\tSp_receiving\t\tReceiving_position\n")

    return 0

def write_report(path_report : str, transfered : List[origin.Conclusion]):
    """
    Write the report following initialized format.
    """
    with open(path_report, 'a+') as file:
        for conclusion in transfered :
            file.write(f"{conclusion.sender_found}\t\t{conclusion.position_sender}\t\t{conclusion.receiver}\t\t{conclusion.position_receiver}\n")
    return 0

def ctrl_removal(base_dir : str, report_file : str):
    """
    Check for already present outputs and delete them to avoid error throwing.
    """
    if report_file in os.listdir(base_dir):
        os.remove(os.path.join(base_dir, report_file))
        print("  Output directory cleaned.\n")

    return 0

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyse the computed signatures and backtrack their origine from the database.")

    parser.add_argument("json_transfer_summary", help='The Path to the summary in .json format generated by the genome signature computation program. Must be in ./output/transfer_summary/')
    parser.add_argument("database_name", help="Path to the directory to analyze (whom signature have been computed). Must be in ./input/sequence_db/ or ./output/output_generator/")
    parser.add_argument("-o", '--output_file', help="Choose the name of the output file. Its destination is ./output/analysis/", type=str, default="analysis_report.txt")
    parser.add_argument('-r', '--reference_file', help="Path to the control file. Evaluate the results of the backtracking respect to it.", type=str, default=None)
    parser.add_argument('-bi','--bootstrapp_iter', help="Choose boostrap parameters for signature backtracking, starting with the number of boostrap iterations (default : 100).", type=int, default=100)
    parser.add_argument('-bk','--bootstrapp_kmer_size', help="Choose kmer size used for the bootstrapp. Default length is 8.", type=int, default=8)
    parser.add_argument('-bw','--bootstrapp_window_size', help="Choose the window size used for the bootstrap. Default length is 5000 (same as for signature computation).", type=int, default=5000)

    args = parser.parse_args()

    json_file = args.json_transfer_summary
    db_dir_name = args.database_name
    report_name = args.output_file
    eval_file_path = args.reference_file

    bs_iter = args.bootstrapp_iter
    bs_km_size = args.bootstrapp_kmer_size
    bs_window_size = args.bootstrapp_window_size

    time_ = []

    dir_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))

    json_path = os.path.join(dir_path, "output", "transfer_summary", json_file)
    db_path = os.path.join(dir_path, "input", "sequence_db", db_dir_name)
    output_path = os.path.join(dir_path, "output", "analysis")
    report_output_path = os.path.join(output_path, report_name)


    print("Loading transfer summary")
    ctrl_removal(output_path, report_name)
    init_report(report_output_path)

    print("Bootstrapping Database Signatures...\n")
    Bootstrapped_signature = origin.load_bootstrapped_db(db_path, window_size=bs_window_size, kmer_size=bs_km_size, Boostrap_iter=bs_iter)
    print("\nBootstrap finished. Starting backtrack analysis...\n")

    for transfer_summary in load.serialize_files(db_path, json_path=json_path):
        st = time()

        top_screen = {transfer_summary.strain : origin.screen_origins(transfer_summary, Bootstrapped_signature)}
        best_hit_search = origin.sliding_window_search(top_screen, db_path, transfer_summary)

        write_report(report_output_path, best_hit_search)
        time_.append(time()-st)

    print("Average runtime per genome", round(np.average(time_)))
    print("Total Runtime", round(sum(time_)))
    
    if eval_file_path is not None:
        accuracy, n_true, num_rep, liste_TP = eval.compare_files(report_output_path, eval_file_path, window_size=bs_window_size)

        eval_report = os.path.join(output_path, "eval_report.txt")

        ctrl_removal(output_path, "eval_report.txt")
        print(f"{accuracy}% ({n_true}/{num_rep}) of the HGT from the reference file have been found.")
        write_report(eval_report, liste_TP)
    
    print("Starting graph computation")
